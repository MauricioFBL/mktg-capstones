# Data Analytics Pipeline

## üìå Description
This repository contains an end-to-end data analytics pipelines for data analytics, leveraging AWS and Airflow (MWAA) for orchestration. The project enables data integration, storage, transformation, and visualization


## üì¶ Key Components

| Component         | Description                                                               |
|------------------|---------------------------------------------------------------------------|
| `Airflow on EC2` | Main orchestrator, deployed via Docker Compose on an EC2 instance.        |
| `Terraform`      | Defines and manages all infrastructure (EC2, roles, S3, Glue Jobs, etc.)   |
| `Glue Jobs`      | PySpark-based ETL scripts for transformation and aggregation.              |
| `Athena`         | Query engine for processed data.                                           |
| `GitHub Actions` | Automates DAG deployment and infrastructure validation/apply pipelines.    |



## üìÇ Repository Structure
```
marketing-analytics-pipeline/
‚îÇ‚îÄ‚îÄ dags/                     # Airflow (MWAA) DAGs for orchestration
‚îÇ‚îÄ‚îÄ scripts/                   # Data generation and ETL scripts
‚îÇ‚îÄ‚îÄ config/                    # Configurations and credentials (DO NOT include real credentials)
‚îÇ‚îÄ‚îÄ notebooks/                 # Jupyter Notebooks for exploratory analysis
‚îÇ‚îÄ‚îÄ sql/                       # SQL queries for analysis
‚îÇ‚îÄ‚îÄ reports/                   # Generated reports and dashboards
‚îÇ‚îÄ‚îÄ visualization/             # Visualization scripts with QuickSight or Power BI
‚îÇ‚îÄ‚îÄ docs/                      # Project documentation
‚îÇ‚îÄ‚îÄ tests/                     # Unit and integration tests
‚îú‚îÄ‚îÄ jobs/
‚îÇ   ‚îú‚îÄ‚îÄ social-media-mktg/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ social-media-big-tbl.py  # C√≥digo del Glue Job
‚îÇ   ‚îî‚îÄ‚îÄ ...   
‚îÇ‚îÄ‚îÄ build/                     # lambda functions zip packahes
‚îÇ‚îÄ‚îÄ functions/                 # Lambda functions
‚îÇ       ‚îî‚îÄ‚îÄ function/
‚îÇ           ‚îî‚îÄ‚îÄ lambda_function.py
‚îú‚îÄ‚îÄ infra/
‚îÇ   ‚îî‚îÄ‚îÄ terraform/
‚îÇ       ‚îú‚îÄ‚îÄ main.tf                  # Entrada general (proveedores, backend)
‚îÇ       ‚îú‚îÄ‚îÄ variables.tf             # Variables globales
‚îÇ       ‚îú‚îÄ‚îÄ terraform.tfvars         # Valores espec√≠ficos del entorno
‚îÇ       ‚îú‚îÄ‚îÄ outputs.tf               # Valores de salida √∫tiles
‚îÇ       ‚îú‚îÄ‚îÄ backend.tf               # (opcional) configuraci√≥n del backend remoto
‚îÇ       ‚îú‚îÄ‚îÄ provider.tf              # (opcional) definici√≥n de proveedor AWS
‚îÇ       ‚îú‚îÄ‚îÄ glue_jobs.tf             # Declaraci√≥n de todos los Glue Jobs
‚îÇ       ‚îú‚îÄ‚îÄ lambdas.tf               # Declaraci√≥n de funciones Lambda
‚îÇ       ‚îú‚îÄ‚îÄ mwaa.tf                  # Declaraci√≥n de entorno MWAA (m√°s adelante)
‚îÇ       ‚îú‚îÄ‚îÄ iam.tf                   # Roles y pol√≠ticas IAM
‚îÇ       ‚îú‚îÄ‚îÄ s3_objects.tf            # Carga de scripts Glue/Lambda a S3
‚îÇ       ‚îî‚îÄ‚îÄ .terraform/              # Plugins (IGNORAR en git)
‚îÇ       ‚îî‚îÄ‚îÄ .terraform.lock.hcl      # Lock de proveedores (‚úÖ versionar)
‚îÇ
‚îÇ‚îÄ‚îÄ config/      
‚îÇ‚îÄ‚îÄ .gitignore                 # Files to ignore in Git
‚îÇ‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îÇ‚îÄ‚îÄ Dockerfile                 # Docker setup
‚îÇ‚îÄ‚îÄ setup.py                   # Custom package installation
```
---
## Airflow Architecture
```mermaid
graph TD
  subgraph VPC
    WebServer[EC2 t3.medium - Webserver]
    Scheduler[EC2 t3.micro - Scheduler]
    RDS[(RDS PostgreSQL)]
    S3[[S3: fcorp-data-prod]]
  end

  WebServer -->|SQLAlchemy Conn| RDS
  Scheduler -->|SQLAlchemy Conn| RDS
  WebServer -->|Mount DAGs/Plugins/Logs| S3
  Scheduler -->|Mount DAGs/Plugins/Logs| S3
```
---

## üõ†Ô∏è Execution Flow

```mermaid
flowchart TD
    subgraph GitHub Actions
        A[Push/Pull Request a dev/main] --> B[Ruff Linter]
        B --> C{¬øSin errores?}
        C -- S√≠ --> D[Terraform Plan]
        D --> E{¬øSin destrucciones?}
        E -- S√≠ --> F[Terraform Apply]
        F --> G[Deploy DAGs a EC2]
        G --> H[Copiar DAG al contenedor Airflow]
        H --> I[Reiniciar Webserver]
        I --> J[Verificar DAGs cargados]
    end

subgraph Airflow Runtime
        J --> K[Ejecutar DAG]
        K --> L[Ejecutar Glue Job]
        L --> M[Escribir datos en S3 particionado]
        M --> N[Crear/Actualizar tabla Athena]
        N --> O[Consulta de datos desde Athena]
    end
```

---

## ‚öôÔ∏è Infrastructure with Terraform

- Modular and scalable (`iam.tf`, `glue_jobs.tf`, `ec2.tf`, etc.)
- Deployment commands:
  ```bash
  terraform init
  terraform plan
  terraform apply
  ```

- Secrets and variables (e.g., `TF_VAR_public_key`) are passed via GitHub Actions Secrets or `terraform.tfvars`.

- Example Glue Job definition:
  ```hcl
  resource "aws_glue_job" "data_transform" {
    name     = "data-transform-job"
    script_location = "s3://${var.bucket}/scripts/glue/transform.py"
    ...
  }
  ```

---



## üîÅ GitHub Actions Automation

### DAG Deployment Workflow

- Runs on push to `main`
- Steps:
  - Ruff linter validation
  - SCP to EC2 using `appleboy/scp-action`
  - Copy DAGs into the Docker container
  - Restart Airflow webserver
  - Check for DAG loading errors

### Terraform Plan & Apply Workflow

- Runs on pull request to `main`
- Steps:
  - Executes `terraform plan`
  - Highlights destructive changes
  - If approved or no changes, applies infrastructure with `terraform apply`

---

## ‚ú≥Ô∏è Adding a New Glue Job

1. Place the script in `jobs/{folder}/my_glue_job.py`
2. Upload via Terraform:

```hcl
resource "aws_s3_object" "glue_script" {
  bucket = var.glue_bucket
  key    = "scripts/glue/my_glue_job.py"
  source = "../../jobs/{folder}/my_glue_job.py"
}
```

3. Register the Glue Job:

```hcl
resource "aws_glue_job" "my_job" {
  name     = "my-glue-job"
  role_arn = aws_iam_role.glue_role.arn

  command {
    name            = "glueetl"
    script_location = "s3://${var.glue_bucket}/scripts/glue/my_glue_job.py"
    python_version  = "3"
  }

  glue_version       = "4.0"
  worker_type        = "G.1X"
  number_of_workers  = 2

  default_arguments = {
    "--INPUT_PATH"  = "s3://fcorp-data-prod/raw/..."
    "--OUTPUT_PATH" = "s3://fcorp-data-prod/consumption/..."
  }
}
```

---

## üìå Best Practices

- Ensure all numerical columns (`campaign_id`, `impressions`, etc.) are stored with correct data types (`int`, `bigint`) in Parquet.
- Athena DDL must match the final structure emitted by Glue.
- Use `skip.header.line.count='1'` if reading CSVs.

---

## ü§ù Contributing

1. Create a new branch from `dev`
2. Open a pull request
3. Ensure:
   - Linter passes
   - Terraform plan is clean
   - DAGs load correctly in Airflow

---


## üöÄ Technologies Desaired to be Used
- **AWS Lambda, S3, Glue, Athena** for data storage and processing.
- **Redshift, Snowflake** for structured data storage.
- **Power BI, QuickSight** for visualization and data analysis.
- **APIs from Meta, Google Ads, DV360, Snapchat, TikTok** for data ingestion.
- **Python, SQL** for data manipulation and transformation.

## üìä Pipeline Workflow
1. **Data Ingestion**: Fetching campaign data via APIs.
2. **Storage**: Saving data in S3 and analytical databases.
3. **Transformation**: Cleaning and structuring using Glue/Athena.
4. **Loading**: Inserting data into Redshift/Snowflake for analysis.
5. **Visualization**: Creating dashboards in Power BI/QuickSight.

## üõ† Installation & Setup
### üìå Prerequisites
- Python 3.8+
- AWS CLI configured
- Docker (optional, for local development)
- Airflow (if running locally instead of MWAA)

### üì• Installation
```bash
# Clone the repository
git clone https://github.com/your_user/marketing-analytics-pipeline.git
# Install dependencies
pip install -r requirements.txt
```


```bash
# install docker
docker ps -al
docker build .
docker image ls 
sh init_docker.sh
```
export TF_VAR_public_key="$(cat ~/.ssh/airflow-key-no-pass.pub)"


## üìú License
This project is licensed under MIT. See [LICENSE](LICENSE) for more details.